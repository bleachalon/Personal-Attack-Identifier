{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Attack Identifier - Hao Qin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset from Wikipedia \n",
    "Q a-i is not in the alphabetical order but the order of classifer implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Improvements\n",
    "1. Used columnTransformer instead of featureUnion.\n",
    "2. Used array of scoring to print out comprehensive result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Metrics & CrossValidation\n",
    "1. Metrics tells me more information about my classifier. For example, recall tells me what percent of personal attack my classifier can caught, and precision can tell me the correctness of my model.\n",
    "2. In my opinion we do not want to miss bad comments since Wikipedia is open to all ages. Based on this we want to lower the false positive rate. **Therefore this model is going to put slightly more weight on Recall, then AUC, then Precision.**\n",
    "3. CrossValidationï¼šYes I think crossvalidation is necessary for decrease sample bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "# download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "# download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_table('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Text clean up\n",
    "1. The original schema that delete 'newline' and 'tab' \n",
    "2. clean up punctuation which improve performance a little but decrease slightly in scores.\n",
    "3. clean up stopwords which actually cause slightly decreasing in scores\n",
    "4. **clean up digits and all to lowercase improve all scores by 0.001% (chosen)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \"\"))\n",
    "comments.comment = comments.comment.apply(lambda x: x.lower())\n",
    "comments.comment = comments.comment.apply(lambda x: x.translate(str.maketrans('','',string.digits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279                 iraq is not good  ======  usa is bad \n",
       "2702703    ____fuck off you little asshole. if you want t...\n",
       "4632658         i have a dick, its bigger than yours! hahaha\n",
       "6545332    == renault ==you sad little bpy for driving a ...\n",
       "6545351    == renault ==you sad little bo for driving a r...\n",
       "7977970    ,  nov  (utc)::because you like to accuse me o...\n",
       "8359431    `::you are not worth the effort. you are argui...\n",
       "8724028    yes, complain to your rabbi and then go shoot ...\n",
       "8845700                     i am using the sandbox, ass wipe\n",
       "8845736    == god damn ==god damn it fuckers, i am using ...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8827073120209901\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "attack = comments['attack']\n",
    "baseline = 1-attack.mean()\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case\n",
    "train = comments.query(\"split=='train'\")\n",
    "test = comments.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.Tuning Parameter\n",
    "1. **C:I tried 0.25, 0.5, 1, 1.5, 2. AUC and Precision increase with C value going up, decrease with C going down. Recall score is the other way around. After combination of test I decide to go with value=0.6 which create a good balance between scores. (chosen)**\n",
    "2. loss: 'hinge' decrease all scores.\n",
    "3. dual, intercept_scaling, max_iter: These parameters does not have obvious effects on scores.\n",
    "4. **TfidfVectorizer: Maxfeature=10000 and n_gram=1 also improves the socres (chosen)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param for gridseachCV\n",
    "pipe_parms=[{\n",
    "    'union__transformer_weights': [{'non-text': 0.2, 'text':0.8}],\n",
    "    'union__text__max_features':[10000],\n",
    "    'union__text__ngram_range': [(1,1)],\n",
    "    'lsvc__C': [0.6],\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.Feature Extraction\n",
    "1. year: no significant improvement showing. With increasing weight add to year auc actually went down.\n",
    "2. logged_in: no significant improvement showing. With increasing weight add to year auc actually went down.\n",
    "3. **sample: slight improvement in recall with 0.2 weight. Increasing weight of sample rise recall by 1-5% however, precision and AUC decrease significantly. After different weight testing the best weight distribution so far is \"sample\": 0.2, \"comment\": 0.8 (chosen)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier & feature extraction\n",
    "pipeline = Pipeline([\n",
    "    ('union', ColumnTransformer([\n",
    "        ('non-text', OneHotEncoder(categories='auto'),['sample']),\n",
    "         ('text', TfidfVectorizer(), 'comment')])\n",
    "    ),\n",
    "    ('lsvc', LinearSVC()) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.Model Selection\n",
    "1. LogisticRegression: This method achieves AUC with around 95.7% and 91% precision. However recall is only 55%.\n",
    "2. MultinomialNB: This method has highest AUC with 96%, but recall and precision are 51% and 87% respectively.\n",
    "3. RandomForestClassifier: This method only has AUC 86%, recall 43%, precision 81%, and takes way longer time then other methods\n",
    "4. MLPClassifier: Could not finish running this method in hours, this is not a suitable method for large scale problem.\n",
    "5. **LinearSVC: This method can achieve AUC 96%, recall 64%, precision 87%. (chosen)**\n",
    "6. SVC: Spent hours and couldn't finish running, so abandoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('union',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('non-text',\n",
       "                                                                         OneHotEncoder(categorical_features=None,\n",
       "                                                                                       categories='auto',\n",
       "                                                                                       drop=None,\n",
       "                                                                                       dtype=<class 'numpy.float64'>,\n",
       "                                                                                       handle_unknown='error',\n",
       "                                                                                       n_values=None,...\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'lsvc__C': [0.6],\n",
       "                          'union__text__max_features': [10000],\n",
       "                          'union__text__ngram_range': [(1, 1)],\n",
       "                          'union__transformer_weights': [{'non-text': 0.2,\n",
       "                                                          'text': 0.8}]}],\n",
       "             pre_dispatch='2*n_jobs', refit='AUC', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score),\n",
       "                      'Precision': 'precision', 'Recall': 'recall'},\n",
       "             verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': metrics.make_scorer(metrics.accuracy_score),\n",
    "           'Recall': 'recall', 'Precision': 'precision'}\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid = pipe_parms, cv=5, scoring=scoring, refit='AUC', return_train_score=True)\n",
    "%time gs.fit(comments, comments['attack'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.Reults\n",
    "1. AUC:96.07%, Recall:65.47%, Precision:87.3%.\n",
    "2. Compare to strawman AUC:0.37%, Recall:10.32%, Precision: -4%.\n",
    "3. LinearSVC produced the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([5.83453908]),\n",
       " 'std_fit_time': array([0.57147617]),\n",
       " 'mean_score_time': array([5.17539859]),\n",
       " 'std_score_time': array([0.76853994]),\n",
       " 'param_lsvc__C': masked_array(data=[0.6],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_union__text__max_features': masked_array(data=[10000],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_union__text__ngram_range': masked_array(data=[(1, 1)],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_union__transformer_weights': masked_array(data=[{'non-text': 0.2, 'text': 0.8}],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'lsvc__C': 0.6,\n",
       "   'union__text__max_features': 10000,\n",
       "   'union__text__ngram_range': (1, 1),\n",
       "   'union__transformer_weights': {'non-text': 0.2, 'text': 0.8}}],\n",
       " 'split0_test_AUC': array([0.9549124]),\n",
       " 'split1_test_AUC': array([0.96438798]),\n",
       " 'split2_test_AUC': array([0.96277687]),\n",
       " 'split3_test_AUC': array([0.96223784]),\n",
       " 'split4_test_AUC': array([0.95902289]),\n",
       " 'mean_test_AUC': array([0.96066761]),\n",
       " 'std_test_AUC': array([0.00336404]),\n",
       " 'rank_test_AUC': array([1]),\n",
       " 'split0_train_AUC': array([0.98475615]),\n",
       " 'split1_train_AUC': array([0.98356572]),\n",
       " 'split2_train_AUC': array([0.98397244]),\n",
       " 'split3_train_AUC': array([0.98365193]),\n",
       " 'split4_train_AUC': array([0.98420056]),\n",
       " 'mean_train_AUC': array([0.98402936]),\n",
       " 'std_train_AUC': array([0.00042853]),\n",
       " 'split0_test_Accuracy': array([0.94558322]),\n",
       " 'split1_test_Accuracy': array([0.9488629]),\n",
       " 'split2_test_Accuracy': array([0.94899236]),\n",
       " 'split3_test_Accuracy': array([0.94635999]),\n",
       " 'split4_test_Accuracy': array([0.94691869]),\n",
       " 'mean_test_Accuracy': array([0.94734344]),\n",
       " 'std_test_Accuracy': array([0.00136189]),\n",
       " 'rank_test_Accuracy': array([1]),\n",
       " 'split0_train_Accuracy': array([0.95903594]),\n",
       " 'split1_train_Accuracy': array([0.95798945]),\n",
       " 'split2_train_Accuracy': array([0.95866913]),\n",
       " 'split3_train_Accuracy': array([0.95807576]),\n",
       " 'split4_train_Accuracy': array([0.958648]),\n",
       " 'mean_train_Accuracy': array([0.95848365]),\n",
       " 'std_train_Accuracy': array([0.00039422]),\n",
       " 'split0_test_Recall': array([0.62693157]),\n",
       " 'split1_test_Recall': array([0.64753495]),\n",
       " 'split2_test_Recall': array([0.68395879]),\n",
       " 'split3_test_Recall': array([0.63833701]),\n",
       " 'split4_test_Recall': array([0.62987491]),\n",
       " 'mean_test_Recall': array([0.64532758]),\n",
       " 'std_test_Recall': array([0.02060737]),\n",
       " 'rank_test_Recall': array([1]),\n",
       " 'split0_train_Recall': array([0.71532377]),\n",
       " 'split1_train_Recall': array([0.70989698]),\n",
       " 'split2_train_Recall': array([0.71054084]),\n",
       " 'split3_train_Recall': array([0.70897719]),\n",
       " 'split4_train_Recall': array([0.71256439]),\n",
       " 'mean_train_Recall': array([0.71146063]),\n",
       " 'std_train_Recall': array([0.00226277]),\n",
       " 'split0_test_Precision': array([0.87339826]),\n",
       " 'split1_test_Precision': array([0.88575742]),\n",
       " 'split2_test_Precision': array([0.85197067]),\n",
       " 'split3_test_Precision': array([0.86967419]),\n",
       " 'split4_test_Precision': array([0.88429752]),\n",
       " 'mean_test_Precision': array([0.87301951]),\n",
       " 'std_test_Precision': array([0.01219703]),\n",
       " 'rank_test_Precision': array([1]),\n",
       " 'split0_train_Precision': array([0.91720722]),\n",
       " 'split1_train_Precision': array([0.91250887]),\n",
       " 'split2_train_Precision': array([0.91865858]),\n",
       " 'split3_train_Precision': array([0.9143535]),\n",
       " 'split4_train_Precision': array([0.91626257]),\n",
       " 'mean_train_Precision': array([0.91579815]),\n",
       " 'std_train_Precision': array([0.00215822])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result\n",
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lsvc__C': 0.6,\n",
       " 'union__text__max_features': 10000,\n",
       " 'union__text__ngram_range': (1, 1),\n",
       " 'union__transformer_weights': {'non-text': 0.2, 'text': 0.8}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9606676083422383"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best score\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20284,   138],\n",
       "       [  852,  1904]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion metrics\n",
    "y_pred=gs.predict(test)\n",
    "metrics.confusion_matrix(test['attack'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "    [[20284,   138]\n",
    "    [  852,  1904]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H&I. Interesting & Difficult things of project\n",
    "1. Overall the whole mechian learning project is one of the most interesting project I've had so far.\n",
    "2. The most interesting part is understanding models and trying to figure out how can I improve the result. The creators of these models and sklearn are all so genius. I learned a lot form their model and way of thinking. These project reminds me how profund computer science is.\n",
    "3. The hardest part is also trying to understand and improve calssifier. Some models and their parameters are hard to understand. Therefore, what I did was trying different combination(so many combinations) to guess what would be the best one and trying to understand them along the way.\n",
    "4. There are a lot of other things that may improve my classifier but beyond my knowleadge at this point, which I will look in to in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
